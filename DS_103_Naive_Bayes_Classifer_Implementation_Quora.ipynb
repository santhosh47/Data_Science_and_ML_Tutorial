{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HK6z39KfK2mc"
   },
   "source": [
    "Importing the necessary libraries and data.\n",
    "\n",
    "Download the data set from here : https://www.kaggle.com/c/quora-insincere-questions-classification/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kgCn9M9vxE3_"
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "train = pd.read_csv('./drive/My Drive/train.csv')\n",
    "test = pd.read_csv('./drive/My Drive/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NzxUO92tKzRd"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "Q3C0HP6bx_by",
    "outputId": "f89d96f2-5e1c-483e-96d2-d6b5e9bb7e64"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>188974</th>\n",
       "      <td>24f478324bc328608830</td>\n",
       "      <td>Which hairstyle suits a thin and parrot nosy 1...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>969067</th>\n",
       "      <td>bdddffacb23411200857</td>\n",
       "      <td>As a Brit that admires American conservatism, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355688</th>\n",
       "      <td>45b8639338af0d29358d</td>\n",
       "      <td>What are the best ways to use Slack in board w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1121566</th>\n",
       "      <td>dbc628b2821848b7edd4</td>\n",
       "      <td>Is there a way to make the name you go by, rat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>819998</th>\n",
       "      <td>a0ac336114d699926985</td>\n",
       "      <td>Is eating pork harmful?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          qid  ... target\n",
       "188974   24f478324bc328608830  ...      0\n",
       "969067   bdddffacb23411200857  ...      1\n",
       "355688   45b8639338af0d29358d  ...      0\n",
       "1121566  dbc628b2821848b7edd4  ...      0\n",
       "819998   a0ac336114d699926985  ...      0\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "T_UkP5ATx_1I",
    "outputId": "4d052f2b-100c-4fe8-ae77-687fa4df8532"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train  (1306122, 3)\n",
      "Shape of test  (375806, 2)\n"
     ]
    }
   ],
   "source": [
    "print ('Shape of train ',train.shape)\n",
    "print ('Shape of test ',test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v0HtRUNrK8eU"
   },
   "source": [
    "Now we'll see what are sincere questions look like and how are insincere questions look like.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "VMt2tefgyC8I",
    "outputId": "a0317076-e653-4bd8-aff9-7bf9d0e64f1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taking a look at Insincere Questions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "544746    What happened to Kim Jong-un that he decides t...\n",
       "336329    Quora has an answer for anything I can imagine...\n",
       "681144    Explaining why colonization contributed to ret...\n",
       "250442    How powerful is the Jewish community in media ...\n",
       "852460     Why does Trump always sit like he's on a toilet?\n",
       "Name: question_text, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print ('Taking a look at Sincere Questions')\n",
    "train.loc[train['target'] == 0].sample(5)['question_text']\n",
    "\n",
    "print ('Taking a look at Insincere Questions')\n",
    "train.loc[train['target'] == 1].sample(5)['question_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r0q4T32ZLEg1"
   },
   "source": [
    "Insincere questions are questions spreading hatred against a group of people or is not real. An insincere question has a value 1 while a sincere question has target value 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-O57j78TyEs8",
    "outputId": "407b3f81-2857-4e92-f3b8-0f6128ccedf4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the attachment to guns by Americans akin to an addiction like it would be with drugs?\n"
     ]
    }
   ],
   "source": [
    " \n",
    "samp = train.sample(1)\n",
    "sentence = samp.iloc[0]['question_text']\n",
    "print (sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WcRyG6D9LKFH"
   },
   "source": [
    "Text Preprocessing in Python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z9NNcZrfLN_8"
   },
   "source": [
    "Removing Numbers and Punctuations. We use regex expressions to remove numbers.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "DjNbQUbhyGz2",
    "outputId": "de801246-87e8-4548-e852-62033f0058af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence After removing numbers\n",
      " Is the attachment to guns by Americans akin to an addiction like it would be with drugs?\n",
      "Sentence After Removing Punctuations\n",
      " Is the attachment to guns by Americans akin to an addiction like it would be with drugs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "sentence = re.sub(r'\\d+','',sentence)\n",
    "print ('Sentence After removing numbers\\n',sentence)\n",
    "\n",
    "#Removing Punctuations in a string.\n",
    "\n",
    "import string\n",
    "sentence = sentence.translate(sentence.maketrans(\"\",\"\",string.punctuation))\n",
    "print ('Sentence After Removing Punctuations\\n',sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hiaJ797VLlKo"
   },
   "source": [
    "Removing Stop Words\n",
    "Stop words are the most common words in a language like “the”, “a”, “on”, “is”, “all”. These words do not carry important meaning and are usually removed from texts.\n",
    "\n",
    "Stop words can be removed by using Natural Language Toolkit (NLTK). NLTK is a set of libraries for symbolic ans statistical natural language processing. It was developed by University of Pennysylvania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "xNqyNUohyImt",
    "outputId": "0b8f45d4-04b5-42f5-ff09-0d61d6121ef2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "['drugs', 'Americans', 'would', 'Is', 'attachment', 'guns', 'addiction', 'akin', 'like']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "words_in_sentence = list(set(sentence.split(' ')) - stop_words)\n",
    "print (words_in_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6BWpWjEnLpXb"
   },
   "source": [
    "Stemming of Words\n",
    "Stemming is the process for reducing derived words to their stem, base or root form—generally a written word form. Ex: owed -> owe muliply -> multipli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "JV4TVa81yKMA",
    "outputId": "df06d6dd-e4a0-4c0d-f681-e9f438f75279"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "['drug', 'american', 'would', 'Is', 'attach', 'gun', 'addict', 'akin', 'like']\n",
      "['drug', 'american', 'would', 'Is', 'attach', 'gun', 'addict', 'akin', 'like']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('wordnet')\n",
    "stemmer= PorterStemmer()\n",
    "for i,word in enumerate(words_in_sentence):\n",
    "    words_in_sentence[i] = stemmer.stem(word)\n",
    "print (words_in_sentence)    \n",
    "\n",
    "#Lemmatization of Words\n",
    "#Lemmatisation is the process of grouping together the different inflected forms of a word so they can be analysed as a single item. Ex: dogs -> dog. I am not clear with difference between lemmatization and stemming. In most of the tutorials, I found them both and I could not understand the clear difference between the two.\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "words = []\n",
    "for i,word in enumerate(words_in_sentence):\n",
    "    words_in_sentence[i] = lemmatizer.lemmatize(word)\n",
    "print (words_in_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ObVV7emrMFbR"
   },
   "source": [
    "**Constructing a Naive Bayes Classifier from Scratch.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y53_i9lyyP4o"
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(train, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2HID08ndMOYI"
   },
   "source": [
    "The next step are as follows:\n",
    "\n",
    "1. Combine all the preprocessing techniques and create a dictionary of words and each word's count in training data.\n",
    "\n",
    "2. Calculate probability for each word in a text and filter the words which has probability less than threshold probability. Words with probability less than threshold probability are insignificant.\n",
    "\n",
    "3. Then for each word in the dictionary, I am creating a probability of that word being in insincere questions and its probability in sincere questions. I am finding the conditional probability to use in naive bayes classifier.\n",
    "\n",
    "4. Prediction using condtional probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4oJjzN6TyU5n"
   },
   "outputs": [],
   "source": [
    "word_count = {}\n",
    "word_count_sincere = {}\n",
    "word_count_insincere = {}\n",
    "sincere  = 0\n",
    "insincere = 0 \n",
    "\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer= PorterStemmer()\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ijv52G6sSKP9"
   },
   "source": [
    "Preprocessing training data. I create three dictionaries which hold word count of words occuring in sincere, insincere and overall after preprocessing each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "AdlF4ImfyW5j",
    "outputId": "489d9964-7ebb-4aef-8da0-7203c0c3ec40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "row_count = train.shape[0]\n",
    "for row in range(0,row_count):\n",
    "    insincere += train.iloc[row]['target']\n",
    "    sincere += (1 - train.iloc[row]['target'])\n",
    "    sentence = train.iloc[row]['question_text']\n",
    "    sentence = re.sub(r'\\d+','',sentence)\n",
    "    sentence = sentence.translate(sentence.maketrans(\"\",\"\",string.punctuation))\n",
    "    words_in_sentence = list(set(sentence.split(' ')) - stop_words)\n",
    "    for index,word in enumerate(words_in_sentence):\n",
    "        word = stemmer.stem(word)\n",
    "        words_in_sentence[index] = lemmatizer.lemmatize(word)\n",
    "    for word in words_in_sentence:\n",
    "        if train.iloc[row]['target'] == 0:   #Sincere Words\n",
    "            if word in word_count_sincere.keys():\n",
    "                word_count_sincere[word]+=1\n",
    "            else:\n",
    "                word_count_sincere[word] = 1\n",
    "        elif train.iloc[row]['target'] == 1: #Insincere Words\n",
    "            if word in word_count_insincere.keys():\n",
    "                word_count_insincere[word]+=1\n",
    "            else:\n",
    "                word_count_insincere[word] = 1\n",
    "        if word in word_count.keys():        #For all words. I use this to compute probability.\n",
    "            word_count[word]+=1\n",
    "        else:\n",
    "            word_count[word]=1\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DM5YWM52SA1l"
   },
   "source": [
    "Finding probability for each word in the dictionary.\n",
    "After that eliminating words which are insignificant. Insignificant words are words which have a probability of occurence less than 0.0001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "0Pz_Zm1iyZCo",
    "outputId": "9a271f90-2787-49c0-9b80-df24f5a3ceb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words  165251\n",
      "Minimum probability  1.142270994655314e-07\n",
      "Total words  1580\n"
     ]
    }
   ],
   "source": [
    "\n",
    "word_probability = {}\n",
    "total_words = 0\n",
    "for i in word_count:\n",
    "    total_words += word_count[i]\n",
    "for i in word_count:\n",
    "    word_probability[i] = word_count[i] / total_words\n",
    "\n",
    "#Eliminating words which are insignificant. Insignificant words are words which have a probability of occurence less than 0.0001.\n",
    "print ('Total words ',len(word_probability))\n",
    "print ('Minimum probability ',min (word_probability.values()))\n",
    "threshold_p = 0.0001\n",
    "for i in list(word_probability):\n",
    "    if word_probability[i] < threshold_p:\n",
    "        del word_probability[i]\n",
    "        if i in list(word_count_sincere):   #list(dict) return it;s key elements\n",
    "            del word_count_sincere[i]\n",
    "        if i in list(word_count_insincere):  \n",
    "            del word_count_insincere[i]\n",
    "print ('Total words ',len(word_probability))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G0bIvtALR5Fc"
   },
   "source": [
    "To apply naive bayes algorithm, we have to find conditional probability. Finding the conditional probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WTG6keZsDpfT"
   },
   "outputs": [],
   "source": [
    "\n",
    "total_sincere_words = sum(word_count_sincere.values())\n",
    "cp_sincere = {}  #Conditional Probability\n",
    "for i in list(word_count_sincere):\n",
    "    cp_sincere[i] = word_count_sincere[i] / total_sincere_words\n",
    "\n",
    "total_insincere_words = sum(word_count_insincere.values())\n",
    "cp_insincere = {}  #Conditional Probability\n",
    "for i in list(word_count_insincere):\n",
    "    cp_insincere[i] = word_count_insincere[i] / total_insincere_words\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NgsPl2m3RvHl"
   },
   "source": [
    "Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vnXJrzMDIBtv",
    "outputId": "36b3e24a-8602-4158-dbaf-0f000be27b3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is  94.13991769547326\n"
     ]
    }
   ],
   "source": [
    "row_count = test.shape[0]\n",
    "\n",
    "p_insincere = insincere / (sincere + insincere)\n",
    "p_sincere = sincere / (sincere + insincere)\n",
    "accuracy = 0\n",
    "\n",
    "for row in range(0,row_count):\n",
    "    sentence = test.iloc[row]['question_text']\n",
    "    target = test.iloc[row]['target']\n",
    "    sentence = re.sub(r'\\d+','',sentence)\n",
    "    sentence = sentence.translate(sentence.maketrans(\"\",\"\",string.punctuation))\n",
    "    words_in_sentence = list(set(sentence.split(' ')) - stop_words)\n",
    "    for index,word in enumerate(words_in_sentence):\n",
    "        word = stemmer.stem(word)\n",
    "        words_in_sentence[index] = lemmatizer.lemmatize(word)\n",
    "    insincere_term = p_insincere\n",
    "    sincere_term = p_sincere\n",
    "    \n",
    "    sincere_M = len(cp_sincere.keys())\n",
    "    insincere_M = len(cp_insincere.keys())\n",
    "    for word in words_in_sentence:\n",
    "        if word not in cp_insincere.keys():\n",
    "            insincere_M +=1\n",
    "        if word not in cp_sincere.keys():\n",
    "            sincere_M += 1\n",
    "         \n",
    "    for word in words_in_sentence:\n",
    "        if word in cp_insincere.keys():\n",
    "            insincere_term *= (cp_insincere[word] + (1/insincere_M))\n",
    "        else:\n",
    "            insincere_term *= (1/insincere_M)\n",
    "        if word in cp_sincere.keys():\n",
    "            sincere_term *= (cp_sincere[word] + (1/sincere_M))\n",
    "        else:\n",
    "            sincere_term *= (1/sincere_M)\n",
    "        \n",
    "    if insincere_term/(insincere_term + sincere_term) > 0.5:\n",
    "        response = 1\n",
    "    else:\n",
    "        response = 0\n",
    "    if target == response:\n",
    "        accuracy += 1\n",
    "    \n",
    "print ('Accuracy is ',accuracy/row_count*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7ZDIHSPJP-xk"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "QuoraQuestion_classificaion.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
